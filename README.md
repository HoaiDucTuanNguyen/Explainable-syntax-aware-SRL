# Explainable-syntax-aware-SRL
Semantic Role Labeling (SRL) plays a crucial role in Natural Language Processing (NLP) by extracting Predicate Argument Structures (PAS) to understand event semantics. While previous studies have demonstrated that syntactic knowledge enhances SRL, they rely on manual feature engineering, runtime syntax parsing, and training data for syntax parsing. This paper introduces a novel syntax-aware SRL method that leverages dependency and constituency features via model merging, eliminating the need for manual engineering, separate parsing, and additional training data. Furthermore, to enhance interpretability, we introduce a post-hoc explaining framework that probes SRL modelsâ€™ syntactic knowledge and quantifies the importance of dependency and constituency features in predictions. Using this framework, we assess how much syntactic features influence argument identification and prediction accuracy. Experiments on biomedical data show that our model achieves state-of-the-art performance while significantly reducing processing time. These contributions advance both SRL performance and interpretability, supporting more efficient and explainable biomedical text processing.
