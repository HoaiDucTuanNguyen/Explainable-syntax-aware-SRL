# Explainable-syntax-aware-SRL
Syntactic features such as dependency and constituency are well-established in improving Semantic Role Labeling (SRL) performance. However, existing syntax-aware SRL models remain largely opaque, as current explainable NLP (X-NLP) methods do not probe inter-word syntactic representations, nor do they quantify the contribution of such representations to individual predictions. This lack of transparency is particularly concerning in high-stakes domains such as biomedicine, where reliable interpretability is crucial for ensuring trust in NLP-assisted health-related decision-making. To address this, we propose a novel post-hoc explanation framework that probes inter-word syntactic relations and quantifies their role in SRL predictions, capturing their contribution to both argument span inclusion and, crucially, prediction correctness. Our method also features a unique perturbation technique that removes syntactic features without deleting associated tokens, thereby avoiding semantic distortion, a key limitation of prior approaches. Beyond explanation, we tackle a critical challenge in X-NLP: evaluating explanation faithfulness. We introduce a comprehensive model-intrinsic explanation evaluation method that avoids reliance on subjective, human-labeled judgment data, instead assessing how well our explanations align with actual performance gains from syntax integration across biomedical predicates. To support controlled analysis, we construct a lightweight syntax-aware SRL model via model merging, which eliminates the need for annotated syntax or runtime parsing. While auxiliary, this model achieves competitive performance on biomedical text and reduces inference cost. Most importantly, experimental results show that our explanations strongly correlate with real model behavior, validating their faithfulness and practical utility.
